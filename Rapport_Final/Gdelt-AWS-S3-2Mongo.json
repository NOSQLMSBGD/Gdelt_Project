{"paragraphs":[{"text":"%dep\nz.reset()\nz.load(\"org.mongodb.spark:mongo-spark-connector_2.11:2.2.6\")","user":"anonymous","dateUpdated":"2019-01-24T13:36:09+0100","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res0: org.apache.zeppelin.dep.Dependency = org.apache.zeppelin.dep.Dependency@4c744696\n"}]},"apps":[],"jobName":"paragraph_1548333369988_-257698200","id":"20190118-150551_1969200344","dateCreated":"2019-01-24T13:36:09+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:246"},{"text":"%md\n\nPrérequis : avoir deja mis sur s3 tous les fichiers zips gdelt\n\n\n## IMPORTS des classes utiles\n\nTout jusqu'à spark.sql mais pas mongo.sql à cause des conflits de nom de classe\n","user":"anonymous","dateUpdated":"2019-01-24T15:35:38+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1548337202664_1855660855","id":"20190124-144002_908119131","dateCreated":"2019-01-24T14:40:02+0100","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3845","dateFinished":"2019-01-24T15:35:38+0100","dateStarted":"2019-01-24T15:35:38+0100","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Prérequis : avoir deja mis sur s3 tous les fichiers zips gdelt</p>\n<h2>IMPORTS des classes utiles</h2>\n<p>Tout jusqu&rsquo;à spark.sql mais pas mongo.sql à cause des conflits de nom de classe</p>\n</div>"}]}},{"text":"import sys.process._\nimport java.net.URL\nimport java.io.File\nimport java.nio.file.{Files, StandardCopyOption}\nimport java.net.HttpURLConnection\nimport org.apache.spark.sql\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.input.PortableDataStream\nimport java.util.zip.ZipInputStream\nimport java.io.{BufferedReader, InputStreamReader}\nimport org.apache.spark.rdd\nimport org.bson.Document\n\nval sqlContext = new SQLContext(sc)\n\n\n","user":"anonymous","dateUpdated":"2019-01-24T13:36:09+0100","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import sys.process._\nimport java.net.URL\nimport java.io.File\nimport java.nio.file.{Files, StandardCopyOption}\nimport java.net.HttpURLConnection\nimport org.apache.spark.sql\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.input.PortableDataStream\nimport java.util.zip.ZipInputStream\nimport java.io.{BufferedReader, InputStreamReader}\nimport org.apache.spark.rdd\nimport org.bson.Document\nwarning: there was one deprecation warning; re-run with -deprecation for details\nsqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@68df029e\n"}]},"apps":[],"jobName":"paragraph_1548333369994_46217581","id":"20190118-150556_2015457198","dateCreated":"2019-01-24T13:36:09+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:247"},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1548337261972_794195045","id":"20190124-144101_960953884","dateCreated":"2019-01-24T14:41:01+0100","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3947","text":"%md\n\n## Environnement\n\nLes modifs sur les variables d'environnement / config de spark\nInfos de bucket S3","dateUpdated":"2019-01-24T14:41:37+0100","dateFinished":"2019-01-24T14:41:37+0100","dateStarted":"2019-01-24T14:41:37+0100","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Environnement</h2>\n<p>Les modifs sur les variables d&rsquo;environnement / config de spark<br/>Infos de bucket S3</p>\n</div>"}]}},{"text":"//import org.apache.spark.SparkConf\n\nspark.conf.set(\"spark.driver.maxResultSize\",\"0\") //0 means unlimited\nspark.conf.set(\"spark.driver.memory\", \"2g\")\nspark.conf.set(\"spark.executor.memory\", \"2g\")\n\nspark.conf.set(\"spark.scheduler.mode\",\"FIFO\")\nspark.conf.set(\"spark.speculation\",\"false\")\nspark.conf.set(\"spark.reducer.maxSizeInFlight\",\"64m\") //défaut 48m\nspark.conf.set(\"spark.serializer\",\"org.apache.spark.serializer.KryoSerializer\") //bonne option recommandée\nspark.conf.set(\"spark.kryoserializer.buffer.max\",\"1g\") // valeur par défaut 64Mo !\nspark.conf.set(\"spark.shuffle.file.buffer\",\"64k\") //default 32k\n\nimport spark.implicits._\n\nspark.sparkContext.hadoopConfiguration.set(\"fs.s3a.access.key\", \"XXX\") // mettre votre ID du fichier credentials.csv\nspark.sparkContext.hadoopConfiguration.set(\"fs.s3a.secret.key\", \"YYY\") // mettre votre secret du fichier credentials.csv\nspark.sparkContext.hadoopConfiguration.set(\"fs.s3a.connection.maximum\",\"1000\")","user":"anonymous","dateUpdated":"2019-01-24T14:40:41+0100","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import spark.implicits._\n"}]},"apps":[],"jobName":"paragraph_1548333369995_-959312177","id":"20190118-150728_206248882","dateCreated":"2019-01-24T13:36:09+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:248"},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1548337303635_-276860669","id":"20190124-144143_1059789524","dateCreated":"2019-01-24T14:41:43+0100","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:4043","text":"%md\n## CONSTANTES\n\n(à adapter pour passer d'un environnement local à AWS ou d'un cluster à un autre)","dateUpdated":"2019-01-24T14:42:14+0100","dateFinished":"2019-01-24T14:42:14+0100","dateStarted":"2019-01-24T14:42:14+0100","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>CONSTANTES</h2>\n<p>(à adapter pour passer d&rsquo;un environnement local à AWS ou d&rsquo;un cluster à un autre)</p>\n</div>"}]}},{"text":"//    /!\\  version en local penser à updater au passage sur AWS\nval IP_MASTER = \"127.0.0.1\"\nval MONGO_URI = \"mongodb://\"+ IP_MASTER +\":27017\"+\"/Gdelt.\"\n/*\nval IP_MASTER = \"172.31.33.253\"\nval IP_SLAVE1 = \"172.31.44.85\"\nval IP_SLAVE2 = \"172.31.47.231\"\n\nval MONGO_URI = \"mongodb://\"+ IP_MASTER +\":27017,\"+IP_MASTER +\":27018,\"+IP_SLAVE1 +\":27019,\"+IP_SLAVE1 +\":27020,\"+ IP_SLAVE2 +\":27021,\"+IP_SLAVE2 +\":27022/Gdelt.\"\n**/\n\n\nval MONGO_URI_REQUETE1 = MONGO_URI + \"Requete1\"\nval MONGO_URI_REQUETE2 = MONGO_URI + \"Requete2\"\nval MONGO_URI_REQUETE3 = MONGO_URI + \"Requete3\"\nval MONGO_URI_REQUETE4 = MONGO_URI + \"Requete4\"\n\nval MONGO_URI_REQUETE3BIS = MONGO_URI + \"Requete3b\"\nval MONGO_URI_REQUETE31 = MONGO_URI + \"Requete31\"\nval MONGO_URI_REQUETE32 = MONGO_URI + \"Requete32\"\nval MONGO_URI_REQUETE4BIS = MONGO_URI + \"Requete4b\"\nval MONGO_URI_REQUETE42 = MONGO_URI + \"Requete42\"\nval MONGO_URI_REQUETE5 = MONGO_URI + \"Requete5\"\n\nval MONGO_URI_REQUETE5BIS = MONGO_URI + \"Requete5b\"\n\n\nval LOCAL_DATA_ZIP_PATH = \"s3a://stephane-mulard-telecom-gdelt2018\"\n\nval LOCAL_SRC_CAMEOEVENTCODE = \"s3a://stephane-mulard-telecom-gdelt2018/CAMEO.eventcodes.txt\"\nval LOCAL_SRC_CAMEOCOUNTRYCODE = \"s3a://stephane-mulard-telecom-gdelt2018/CAMEO.country.txt\"\nval LOCAL_SRC_FIPSCOUNTRYCODE = \"s3a://stephane-mulard-telecom-gdelt2018/FIPS.country.txt\"\n\n","user":"anonymous","dateUpdated":"2019-01-24T14:43:20+0100","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"IP_MASTER: String = 127.0.0.1\nMONGO_URI: String = mongodb://127.0.0.1:27017/Gdelt.\nMONGO_URI_REQUETE1: String = mongodb://127.0.0.1:27017/Gdelt.Requete1\nMONGO_URI_REQUETE2: String = mongodb://127.0.0.1:27017/Gdelt.Requete2\nMONGO_URI_REQUETE3: String = mongodb://127.0.0.1:27017/Gdelt.Requete3\nMONGO_URI_REQUETE4: String = mongodb://127.0.0.1:27017/Gdelt.Requete4\nMONGO_URI_REQUETE3BIS: String = mongodb://127.0.0.1:27017/Gdelt.Requete3b\nMONGO_URI_REQUETE31: String = mongodb://127.0.0.1:27017/Gdelt.Requete31\nMONGO_URI_REQUETE32: String = mongodb://127.0.0.1:27017/Gdelt.Requete32\nMONGO_URI_REQUETE4BIS: String = mongodb://127.0.0.1:27017/Gdelt.Requete4b\nMONGO_URI_REQUETE42: String = mongodb://127.0.0.1:27017/Gdelt.Requete42\nMONGO_URI_REQUETE5: String = mongodb://127.0.0.1:27017/Gdelt.Requete5\nMONGO_URI_REQUETE5BIS: String = mongodb://127.0.0.1:27017/Gdelt.Requete5b\nLOCAL_DATA_ZIP_PATH: String = s3a://stephane-mulard-telecom-gdelt2018\nLOCAL_SRC_CAMEOEVENTCODE: String = s3a://stephane-mulard-telecom-gdelt2018/CAMEO.eventcodes.txt\nLOCAL_SRC_CAMEOCOUNTRYCODE: String = s3a://stephane-mulard-telecom-gdelt2018/CAMEO.country.txt\nLOCAL_SRC_FIPSCOUNTRYCODE: String = s3a://stephane-mulard-telecom-gdelt2018/FIPS.country.txt\n"}]},"apps":[],"jobName":"paragraph_1548333369996_503958519","id":"20190118-152612_1675243185","dateCreated":"2019-01-24T13:36:09+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:249"},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1548337400058_1945410074","id":"20190124-144320_1256141385","dateCreated":"2019-01-24T14:43:20+0100","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:4133","text":"%md\n## Petites Fonctions\n- fonctions de cast\n- telechargement de fichiers\n- chargement et Lecture des fichiers de decode pour les dropdown voire les requettes d'affichage\n","dateUpdated":"2019-01-24T14:44:02+0100","dateFinished":"2019-01-24T14:44:02+0100","dateStarted":"2019-01-24T14:44:02+0100","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Petites Fonctions</h2>\n<ul>\n  <li>fonctions de cast</li>\n  <li>telechargement de fichiers</li>\n  <li>chargement et Lecture des fichiers de decode pour les dropdown voire les requettes d&rsquo;affichage</li>\n</ul>\n</div>"}]}},{"text":"// Fonction permettant de télécharger un fichier à partir d'une adresse\ndef fileDownloader(urlOfFileToDownload: String, fileName: String) = {\n    val url = new URL(urlOfFileToDownload)\n    val connection = url.openConnection().asInstanceOf[HttpURLConnection]\n    connection.setConnectTimeout(5000)\n    connection.setReadTimeout(5000)\n    connection.connect()\n\n    if (connection.getResponseCode >= 400)\n        println(\"error\")\n    else\n        url #> new File(fileName) !!\n}\n\n// Fonction qui permet de télécharger les fichiers dans un dossier donné\ndef downloadFilesFromList(listFiles: sql.Dataset[sql.Row], dir: String) = {\n    listFiles.select(\"url\").repartition(100).foreach( r=> {\n            val URL = r.getAs[String](0)\n            val fileName = r.getAs[String](0).split(\"/\").last\n            fileDownloader(URL,  dir + fileName)\n            \n    })\n}\n\ndef readGdeltDataFromFiles(filesPath: String, tableName: String, month : String) : rdd.RDD[String] = {\n    sc.binaryFiles(filesPath + \"/2018\" + month  + \"*.\" + tableName + \".CSV.zip\").\n       flatMap {  // decompresser les fichiers\n           case (name: String, content: PortableDataStream) =>\n// // meme avec ce try ca plante : on laisse tomber tant pis (mais on garde le try sur le contenu des champs)\n//               try{\n                    val zis = new ZipInputStream(content.open)\n                    Stream.continually(zis.getNextEntry).\n                      takeWhile{ case null => zis.close(); false\n                                 case _ => true }.\n                      flatMap { _ =>\n                          val br = new BufferedReader(new InputStreamReader(zis))\n                          Stream.continually(br.readLine()).takeWhile(_ != null)\n                      }\n//                }\n//                catch {\n//                    case _ : Throwable => None\n//                }\n        }\n}\n\n// Fonction qui permet de créer une table des codes events cameo\ndef createCameoEventCodesTable() : String = {\n    sqlContext.read.\n        option(\"delimiter\",\"\\t\").\n        option(\"infer_schema\",\"true\").\n        option(\"header\", \"True\").\n        csv(LOCAL_SRC_CAMEOEVENTCODE).\n        withColumnRenamed(\"_c0\",\"code\").\n        withColumnRenamed(\"_c1\",\"desc\").\n        createOrReplaceTempView(\"cameoEvents\")\n\n    spark.catalog.cacheTable(\"cameoEvents\")\n    return \"cameoEvents\"\n}\n\n// Fonction qui permet de créer une table des codes events cameo\ndef createFIPSCountryCodesTable() : String = {\n    sqlContext.read.\n        option(\"delimiter\",\"\\t\").\n        option(\"infer_schema\",\"true\").\n        option(\"header\", \"False\").\n        csv(LOCAL_SRC_FIPSCOUNTRYCODE).\n        withColumnRenamed(\"_c0\",\"code\").\n        withColumnRenamed(\"_c1\",\"desc\").\n        createOrReplaceTempView(\"fipsCountries\")\n\n    spark.catalog.cacheTable(\"fipsCountries\")\n    return \"fipsCountries\"\n}\n\n// Fonction qui récupère la liste des valeurs d'une table de référence avec 2 colonnes\ndef listcodes(table: String) : Seq[(String, String)] = {\n  spark.sql(\"select * from \" + table).collect.map(x => ((x(0).asInstanceOf[String], x(1).asInstanceOf[String])))\n}\n\ndef getMonths () : Object = {\n     z.select(\"Mois\", \"12\", Seq((\"01\",\"Janvier\"),(\"02\",\"Février\"),(\"03\",\"Mars\"),(\"04\",\"Avril\"),(\"05\",\"Mai\"),(\"06\",\"Juin\"),(\"07\",\"Juillet\"),(\"08\",\"Août\"),(\"09\",\"Septembre\"),(\"10\",\"Octobre\"),(\"11\",\"Novembre\"),(\"12\",\"Décembre\")))\n}\n\ndef getDays() : Object = {\n    var jours = Seq[(String, String)]()\n    for(i<- 1 to 31){\n        jours = jours :+ (f\"${i}%02d\",f\"${i}%02d\")\n    }\n    return z.select(\"Jour\",\"01\", jours)\n}\n\n\n// FONCTIONS de type : solution retenu on laisse passer en forcant une valeur par defaut : l'alternative serait de mettre un -1 ou -100 \n// et filtrer après  mais on n'a deja pas tant de données donc mieux vaut avoir des 0\ndef toDouble(s : String): Double = {\n    if(s.isEmpty) {\n        0\n    }\n    else {\n        try\n        {\n            s.toDouble\n        }\n        catch { case _:  Throwable => 0 }\n    }\n    \n}\n\ndef toInt(s : String): Int = {\n    if(s.isEmpty) {\n        0\n    }\n    else {\n        try\n        {\n            s.toInt\n        }\n        catch { case _:  Throwable => 0 }\n    }\n}\n\ndef toBigInt(s : String): BigInt = {\n    if(s.isEmpty) {\n        BigInt(0)\n    }\n    else {\n        try\n        {\n            BigInt(s)\n        }\n        catch { case _:  Throwable =>  BigInt(0) }\n    }\n}\n","user":"anonymous","dateUpdated":"2019-01-24T15:38:01+0100","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"warning: there was one feature warning; re-run with -feature for details\nfileDownloader: (urlOfFileToDownload: String, fileName: String)Any\ndownloadFilesFromList: (listFiles: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row], dir: String)Unit\nreadGdeltDataFromFiles: (filesPath: String, tableName: String, month: String)org.apache.spark.rdd.RDD[String]\ncreateCameoEventCodesTable: ()String\ncreateFIPSCountryCodesTable: ()String\nlistcodes: (table: String)Seq[(String, String)]\ngetMonths: ()Object\ngetDays: ()Object\ntoDouble: (s: String)Double\ntoInt: (s: String)Int\ntoBigInt: (s: String)BigInt\n"}]},"apps":[],"jobName":"paragraph_1548333369996_18237748","id":"20190118-151211_345692492","dateCreated":"2019-01-24T13:36:09+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:250"},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1548337450680_812142762","id":"20190124-144410_590584034","dateCreated":"2019-01-24T14:44:10+0100","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:4246","text":"%md\n# Classes formats de données\n\npour passage de rdd full du contenu des zip à un dataset avec champs utile uniquement","dateUpdated":"2019-01-24T14:45:39+0100","dateFinished":"2019-01-24T14:45:39+0100","dateStarted":"2019-01-24T14:45:39+0100","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Classes formats de données</h1>\n<p>pour passage de rdd full du contenu des zip à un dataset avec champs utile uniquement</p>\n</div>"}]}},{"text":"case class EventsLoad(\nGLOBALEVENTID: Int, //0\nSQLDATE: Int,       //1\n//MonthYear: Int,   //2\n//Year: Int,        //3\n//FractionDate: Double, //4\nActor1Code: String,  //5\nActor1Name: String,  //6\nActor1CountryCode: String, //7\n//Actor1KnownGroupCode: String, //8\n//Actor1EthnicCode: String, //9\n//Actor1Religion1Code: String, //10\n//Actor1Religion2Code: String, //11\n//Actor1Type1Code: String,  //12\n//Actor1Type2Code: String, //13\n//Actor1Type3Code: String, //14\nActor2Code: String,   //15\nActor2Name: String,   //16\nActor2CountryCode: String, //17\n//Actor2KnownGroupCode: String,  //18\n//Actor2EthnicCode: String, //19\n//Actor2Religion1Code: String, //20\n//Actor2Religion2Code: String, //21\n//Actor2Type1Code: String, //22\n//Actor2Type2Code: String, //23\n//Actor2Type3Code: String, //24\n//IsRootEvent: Int,        //25\nEventCode: String,         //26\n//EventBaseCode: String,   //27\n//EventRootCode: String,   //28\n//QuadClass: Int,          //29\nGoldsteinScale: Double,    //30\nNumMentions: Int,          //31\n//NumSources: Int,         //32\nNumArticles: Int,          //33\nAvgTone: Double,           //34\nActor1Geo_Type: Int,       //35\nActor1Geo_FullName: String, //36\nActor1Geo_CountryCode: String, //37\n//Actor1Geo_ADM1Code: String,  //38\n//Actor1Geo_ADM2Code: String,  //39\nActor1Geo_Lat: Double,      //40\nActor1Geo_Long: Double,     //41\n//Actor1Geo_FeatureID: String, //42\nActor2Geo_Type: Int,        //43\nActor2Geo_FullName: String, //44\nActor2Geo_CountryCode: String, //45\n//Actor2Geo_ADM1Code: String,  //46\n//Actor2Geo_ADM2Code: String,  //47\nActor2Geo_Lat: Double,         //48\nActor2Geo_Long: Double,        //49\n//Actor2Geo_FeatureID: String, //50\nActionGeo_Type: Int,           //51\nActionGeo_FullName: String,    //52\nActionGeo_CountryCode: String, //53\n//ActionGeo_ADM1Code: String,  //54\n//ActionGeo_ADM2Code: String,  //55\nActionGeo_Lat: Double,         //56\nActionGeo_Long: Double,        //57\n//ActionGeo_FeatureID: String, //58\n//DATEADDED: BigInt,           //59\nSOURCEURL: String            //60\n    )\n    \ncase class MentionsLoad(\nGLOBALEVENTID: Int,     //0\n//EventTimeDate: BigInt,  //1\nMentionTimeDate: BigInt,  //2\n//MentionType: Int,       //3\n//MentionSourceName: String,//4\n//MentionIdentifier: String,//5\n//SentenceID: Int,        //6\n//Actor1CharOffset: Int,  //7\n//Actor2CharOffset: Int,  //8\n//ActionCharOffset: Int,  //9\n//InRawText: Int,         //10\n//Confidence: Int,        //11\n//MentionDocLen: Int,     //12\nMentionDocTone: Double,  //13\nMentionDocTranslationInfo: String //14\n//Extras: String          //15\n)","user":"anonymous","dateUpdated":"2019-01-24T13:36:09+0100","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"defined class EventsLoad\ndefined class MentionsLoad\n"}]},"apps":[],"jobName":"paragraph_1548333369997_-1948065111","id":"20190118-164932_1237570205","dateCreated":"2019-01-24T13:36:09+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:251"},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1548337549856_-1245789841","id":"20190124-144549_455218557","dateCreated":"2019-01-24T14:45:49+0100","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:4348","text":"%md\n# Container  pour passer d'une liste de fichier à une table rdd de contenu\n\nnb : lazy donc le simple appel à cette fonction ne fera rien","dateUpdated":"2019-01-24T14:47:22+0100","dateFinished":"2019-01-24T14:47:22+0100","dateStarted":"2019-01-24T14:47:22+0100","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Container pour passer d&rsquo;une liste de fichier à une table rdd de contenu</h1>\n<p>nb : lazy donc le simple appel à cette fonction ne fera rien</p>\n</div>"}]}},{"text":"def read_files_and_create_events_table(moisJour : String) = {\n    \n//On charge les données lues dans les csv dans un RDD\nval allEvents = readGdeltDataFromFiles(LOCAL_DATA_ZIP_PATH, \"export\", moisJour)\n    \n// On type les infos de events et on les écrit dans une table temporaire en mémoire\nallEvents.map(_.split(\"\\t\")).filter(_.length==61).map(\n    e=> EventsLoad(\n              toInt(e(0)),\n              toInt(e(1)),\n              e(5),\n              e(6), \n              e(7), \n              e(15), \n              e(16), \n              e(17), \n              e(26), \n              toDouble(e(30)), \n              toInt(e(31)), \n              toInt(e(33)), \n              toDouble(e(34)), \n              toInt(e(35)), \n              e(36),\n              e(37), \n              toDouble(e(40)),\n              toDouble(e(41)), \n              toInt(e(43)), \n              e(44), \n              e(45), \n              toDouble(e(48)), \n              toDouble(e(49)), \n              toInt(e(51)), \n              e(52), \n              e(53), \n              toDouble(e(56)), \n              toDouble(e(57)),\n              e(60)\n             )\n).toDS.createOrReplaceTempView(\"events\")\n\n//spark.catalog.cacheTable(\"events\")\n\n}\n\ndef read_files_and_create_mentions_table(moisJour : String) = {\n\nval allMentions = readGdeltDataFromFiles(LOCAL_DATA_ZIP_PATH, \"mentions\", moisJour)\n\nallMentions.map(_.split(\"\\t\")).filter(_.length>=13).map(\n    m =>  {\n                if(m.length==15) {\n                    //mentions avec langue\n                    MentionsLoad(toInt(m(0)), toBigInt(m(2)), toDouble(m(13)),m(14).substring(6,9))\n                 }\n                 else \n                    //mentions sans langue, anglais par défaut\n                    MentionsLoad(toInt(m(0)), toBigInt(m(2)), toDouble(m(13)), \"eng\")\n        }\n    ).toDS.createOrReplaceTempView(\"mentions\")\n\n//spark.catalog.cacheTable(\"mentions\")\n\n}","user":"anonymous","dateUpdated":"2019-01-24T13:36:09+0100","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"read_files_and_create_events_table: (moisJour: String)Unit\nread_files_and_create_mentions_table: (moisJour: String)Unit\n"}]},"apps":[],"jobName":"paragraph_1548333369998_657752926","id":"20190118-161853_1083044970","dateCreated":"2019-01-24T13:36:09+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:252"},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1548337649485_-1486526912","id":"20190124-144729_1129903123","dateCreated":"2019-01-24T14:47:29+0100","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:4438","text":"%md\n# Decode\n\nChargement des fichiers de decode\n\nDEPRECATED : à faire coté GUI pas côté chargement","dateUpdated":"2019-01-24T14:48:17+0100","dateFinished":"2019-01-24T14:48:17+0100","dateStarted":"2019-01-24T14:48:17+0100","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Decode</h1>\n<p>Chargement des fichiers de decode</p>\n<p>DEPRECATED : à faire coté GUI pas côté chargement</p>\n</div>"}]}},{"text":"createCameoEventCodesTable()\ncreateFIPSCountryCodesTable()","user":"anonymous","dateUpdated":"2019-01-24T13:36:09+0100","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res73: String = cameoEvents\nres74: String = fipsCountries\n"}]},"apps":[],"jobName":"paragraph_1548333369999_-551576243","id":"20190122-100336_2283525","dateCreated":"2019-01-24T13:36:09+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:253"},{"text":"// sur-fonction pour collecter events ET mentions ... \n// dans la pratique si on ne charge QUE du 2 ou du 5\n// ce n'est pas utile de charger mentions ...\n\ndef read_events_and_mentions_table_ByDate(moisJour : String) = {\n    read_files_and_create_events_table(moisJour)\n    read_files_and_create_mentions_table(moisJour)\n}","user":"anonymous","dateUpdated":"2019-01-24T14:49:48+0100","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"read_events_and_mentions_table_ByDate: (moisJour: String)Unit\n"}]},"apps":[],"jobName":"paragraph_1548333370000_1625597395","id":"20190122-120245_1128883616","dateCreated":"2019-01-24T13:36:10+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:254"},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1548337838434_-1490315039","id":"20190124-145038_1263134239","dateCreated":"2019-01-24T14:50:38+0100","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:4551","text":"%md\n## Libs Mongo\n(ne plus utiliser du code avec apache.spark.sql à partir de ce point)","dateUpdated":"2019-01-24T14:50:46+0100","dateFinished":"2019-01-24T14:50:46+0100","dateStarted":"2019-01-24T14:50:46+0100","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Libs Mongo</h2>\n<p>(ne plus utiliser du code avec apache.spark.sql à partir de ce point)</p>\n</div>"}]}},{"text":"import com.mongodb.spark._\nimport com.mongodb.spark.config._","user":"anonymous","dateUpdated":"2019-01-24T13:36:10+0100","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import com.mongodb.spark._\nimport com.mongodb.spark.config._\n"}]},"apps":[],"jobName":"paragraph_1548333370001_1455638128","id":"20190122-131437_38752288","dateCreated":"2019-01-24T13:36:10+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:255"},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1548337853834_-1411791018","id":"20190124-145053_625105995","dateCreated":"2019-01-24T14:50:53+0100","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:4641","text":"%md\n\n# Requete 1\n\na partir des tables events et mentions supposées disponibles (ou au moins definies si lazy)\n\nRDD > sparksql > mongo\n","dateUpdated":"2019-01-24T14:53:06+0100","dateFinished":"2019-01-24T14:53:06+0100","dateStarted":"2019-01-24T14:53:06+0100","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Requete 1</h1>\n<p>a partir des tables events et mentions supposées disponibles (ou au moins definies si lazy)</p>\n<p>RDD &gt; sparksql &gt; mongo</p>\n</div>"}]}},{"text":"// attention on va perdre les compteurs pour les mensions qui font reference a des events qui ne sont pas dans le même batch !\n// imppossible de garder des grosses fractions genre un mois ou un an vu la memoire :(\n\n// attention on recupere des doublons de \"clé\" de requetage si on a des batches qui ont des dates en commun\n// et c'est generalement le cas : pour 99% d'event du jour meme on a 1% deportés dans des fichiers d'un jour suivant\n// prevoir de recumuler possiblement a l'etape de consolidation globale  ou directement dans la requete client\n\ndef writeRequete1ToMongo(moisJour: String ) = {\n\nval dfRequete1 = spark.sql(\"\"\"\nselect\nev.sqldate as Jour,\nev.ActionGeo_CountryCode as CodePays,\nem.MentionDocTranslationInfo as Langue,\n\ncount(distinct em.GLOBALEVENTID) as NbEvents,\ncount(1) as NbMentions\n\nfrom mentions em join  events ev on ev.GLOBALEVENTID=em.GLOBALEVENTID\n \ngroup by ev.ActionGeo_CountryCode, ev.sqldate, em.MentionDocTranslationInfo\n-- order by Jour, CodePays, Langue\n-- NON PAS DE ORDER ca bouffe du temps pour rien on verra en visu\n-- ce dataframe là il part à la poubelle dès que le batch est fini !\n\"\"\").toDF()\n\nMongoSpark.save(dfRequete1, WriteConfig(Map(\"uri\" -> MONGO_URI_REQUETE1)))\n\nprintln(\"Succès pour écriture Requête 1 (moisjour): \" + moisJour)\n\n}","user":"anonymous","dateUpdated":"2019-01-24T15:00:33+0100","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"writeRequete1ToMongo: (moisJour: String)Unit\n"}]},"apps":[],"jobName":"paragraph_1548333370001_274177343","id":"20190122-102412_1608054279","dateCreated":"2019-01-24T13:36:10+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:256"},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1548338442804_1816735083","id":"20190124-150042_1903248573","dateCreated":"2019-01-24T15:00:42+0100","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:4776","text":"%md\n\n# Requete 2\n\nBatch seul suffit pas besoin de consolidation\npas besoin de mention","dateUpdated":"2019-01-24T15:11:03+0100","dateFinished":"2019-01-24T15:11:03+0100","dateStarted":"2019-01-24T15:11:03+0100","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Requete 2</h1>\n<p>Batch seul suffit pas besoin de consolidation<br/>pas besoin de mention</p>\n</div>"}]}},{"text":"// Ecriture Requete 2\ndef writeRequete2ToMongo(moisJour: String ) = {\n    \n    \n// impossible d'avoir un vrai union all en mongo donc là on doit rester en spark\n// le unwind ne marche que sur un champs liste par sur une collection  ...\n// sauf à tout reprendre avec une phase intermediaire supplémentaire\n\n// On met tout en vrac on drop juste les lignes vides\n\nval dfRequete2 = spark.sql(\"\"\"\nSELECT ev.Actor1Name as actor, ev.sqldate as Jour, ev.SOURCEURL, ev.GLOBALEVENTID, ev.EventCode as EventCode, ev.GoldsteinScale\nfrom  events ev where Actor1Name <> ''\nunion all\n\nSELECT ev.Actor2Name, ev.sqldate, ev.SOURCEURL, ev.GLOBALEVENTID, ev.EventCode, ev.GoldsteinScale\nfrom  events ev where Actor2Name <> ''\n\"\"\").toDF()\n\nMongoSpark.save(dfRequete2, WriteConfig(Map(\"uri\" -> MONGO_URI_REQUETE2)))\n\nprintln(\"Succès pour écriture Requête 2 (moisjour): \" + moisJour)\n\n}","user":"anonymous","dateUpdated":"2019-01-24T15:03:39+0100","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"writeRequete2ToMongo: (moisJour: String)Unit\n"}]},"apps":[],"jobName":"paragraph_1548333370002_-1094771512","id":"20190122-120710_1355653197","dateCreated":"2019-01-24T13:36:10+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:257"},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1548338653939_692880054","id":"20190124-150413_921545135","dateCreated":"2019-01-24T15:04:13+0100","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:4889","text":"%md\nSamples d'appels batch isolés","dateUpdated":"2019-01-24T15:06:58+0100","dateFinished":"2019-01-24T15:06:58+0100","dateStarted":"2019-01-24T15:06:58+0100","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Samples d&rsquo;appels batch isolés</p>\n</div>"}]}},{"text":"// un batch pour le 10 et 11 decembre avec juste requete2\nval moisJour = \"121[01]\"\n\n// read_events_and_mentions_table_ByDate(moisJour)\nread_files_and_create_events_table(moisJour)\n// read_files_and_create_mentions_table(moisJour)\n    \nwriteRequete2ToMongo(moisJour)","user":"anonymous","dateUpdated":"2019-01-24T15:06:51+0100","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"msg":[{"type":"TEXT","data":""}]},"apps":[],"jobName":"paragraph_1548333370003_-813092912","id":"20190122-123343_2126389958","dateCreated":"2019-01-24T13:36:10+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:258"},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1548338824452_-1907008543","id":"20190124-150704_1921994490","dateCreated":"2019-01-24T15:07:04+0100","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:5005","text":"%md\n\n## suite des fonctions de requetes :\n\n## Requete 3 (1er etage batch)\n\nalimentera à la fois le 2e etage de la requete3 et la requete 4","dateUpdated":"2019-01-24T15:10:24+0100","dateFinished":"2019-01-24T15:10:24+0100","dateStarted":"2019-01-24T15:10:24+0100","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>suite des fonctions de requetes :</h2>\n<h2>Requete 3 (1er etage batch)</h2>\n<p>alimentera à la fois le 2e etage de la requete3 et la requete 4</p>\n</div>"}]}},{"text":"// Ecriture Requete 3.1\ndef writeRequete31ToMongo(moisJour: String ) = {\n\n// DOIT ABSOLUMENT N AVOIR QUE DES CHAMPS SOMMABLES (hors champs input)\n// sera re-cumulé plus tard Pour les cumul globaux lors du 2e etage\n\nval dfRequete31 = spark.sql(\"\"\"\nselect\n  tt.Mois, tt.CountryCode, tt.Langue, tt.Actor,\n  tt.ActorCode, tt.ActorCountryCode,\n  sum(if(tt.MentionDocTone>0,1,0)) as nbpos,\n  sum(if(tt.MentionDocTone<0,1,0)) as nbneg,\n  sum(if(tt.MentionDocTone>0,MentionDocTone,0)) as sommepos,\n  sum(if(tt.MentionDocTone<0,-MentionDocTone,0)) as sommeneg,\n  count(1) as nbttl\nfrom\n  (select\n    substr(ev.sqldate,1,6) as Mois,\n    ev.Actor1Geo_CountryCode as CountryCode,\n    em.MentionDocTranslationInfo as Langue,\n    ev.Actor1Name as Actor,\n    ev.Actor1Code as ActorCode, \n    ev.Actor1CountryCode as ActorCountryCode,\n    em.MentionDocTone\n  from mentions em join  events ev on ev.GLOBALEVENTID=em.GLOBALEVENTID\n  where Actor1Name <> '' and Actor1Geo_CountryCode <> ''\n  union all\n  select\n    substr(ev.sqldate,1,6) ,\n    ev.Actor2Geo_CountryCode,\n    em.MentionDocTranslationInfo,\n    ev.Actor2Name,\n    ev.Actor2Code, \n    ev.Actor2CountryCode,\n    em.MentionDocTone\n  from mentions em join  events ev on ev.GLOBALEVENTID=em.GLOBALEVENTID\n  where Actor2Name <> ''  and Actor2Geo_CountryCode <> ''\n)tt\ngroup by tt.Mois, tt.CountryCode, tt.Langue, tt.Actor, tt.ActorCode, tt.ActorCountryCode\n\"\"\").toDF()\n\nMongoSpark.save(dfRequete31, WriteConfig(Map(\"uri\" -> MONGO_URI_REQUETE31)))\n\nprintln(\"Succès pour écriture Requête 31 (moisjour): \" + moisJour)\n\n}","user":"anonymous","dateUpdated":"2019-01-24T15:09:37+0100","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"writeRequete31ToMongo: (moisJour: String)Unit\n"}]},"apps":[],"jobName":"paragraph_1548333370003_1582519842","id":"20190122-123917_751452246","dateCreated":"2019-01-24T13:36:10+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259"},{"user":"anonymous","dateUpdated":"2019-01-24T15:09:47+0100","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"results":{},"enabled":true,"editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1548333370004_393806040","id":"20190123-115445_622855817","dateCreated":"2019-01-24T13:36:10+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:260","text":"%md\n\n## Requete 5","dateFinished":"2019-01-24T15:09:47+0100","dateStarted":"2019-01-24T15:09:47+0100","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Requete 5</h2>\n</div>"}]}},{"text":"// Ecriture Requete 5 (pré cumul : par batch)\ndef writeRequete5ToMongo(moisJour: String ) = {\n    \nval dfRequete5 = spark.sql(\"\"\"\nselect\n  tt.Mois, tt.Actor1, tt.ActorCode1, tt.Actor2, tt.ActorCode2,\n  sum(if(tt.GoldsteinScale>0,1,0)) as nbpos,\n  sum(if(tt.GoldsteinScale<0,1,0)) as nbneg,\n  sum(if(tt.GoldsteinScale>0,GoldsteinScale,0)) as sommepos,\n  sum(if(tt.GoldsteinScale<0,-GoldsteinScale,0)) as sommeneg,\n  count(1) as nbttl\nfrom\n  (select\n    substr(SQLDATE,1,6) as Mois,\n    Actor1Name as Actor1,\n    Actor1Code as ActorCode1,\n    Actor2Name as Actor2,\n    Actor2Code as ActorCode2,\n    GoldsteinScale\n  from events\n  where Actor1Name <> '' and LENGTH(Actor1Code) = 3 and Actor2Name <> '' and LENGTH(Actor2Code) = 3\n  union all\n  select\n    substr(SQLDATE,1,6) as Mois,\n    Actor2Name as Actor1,\n    Actor2Code as ActorCode1,\n    Actor1Name as Actor2,\n    Actor1Code as ActorCode2,\n    GoldsteinScale\n  from events\n  where Actor1Name <> '' and LENGTH(Actor1Code) = 3 and Actor2Name <> '' and LENGTH(Actor2Code) = 3\n  )tt\ngroup by tt.Mois, tt.Actor1, tt.ActorCode1, tt.Actor2, tt.ActorCode2\n\"\"\").toDF()\n\nMongoSpark.save(dfRequete5, WriteConfig(Map(\"uri\" -> MONGO_URI_REQUETE5)))\n\nprintln(\"Succès pour écriture Requête 5 (moisjour): \" + moisJour)\n\n}","user":"anonymous","dateUpdated":"2019-01-24T13:36:10+0100","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"writeRequete5ToMongo: (moisJour: String)Unit\n"}]},"apps":[],"jobName":"paragraph_1548333370005_-120855574","id":"20190123-113827_633750745","dateCreated":"2019-01-24T13:36:10+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:261"},{"text":"// DEBUG\nz.show(spark.sql(\"\"\"\nselect *\n  from events\n  limit 10\n\"\"\"))","user":"anonymous","dateUpdated":"2019-01-24T15:11:26+0100","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"java.lang.RuntimeException: java.lang.reflect.InvocationTargetException\n  at org.apache.zeppelin.spark.SparkZeppelinContext.showData(SparkZeppelinContext.java:112)\n  at org.apache.zeppelin.interpreter.BaseZeppelinContext.show(BaseZeppelinContext.java:238)\n  at org.apache.zeppelin.interpreter.BaseZeppelinContext.show(BaseZeppelinContext.java:224)\n  ... 72 elided\nCaused by: java.lang.reflect.InvocationTargetException: org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input Pattern file:/home/palandir/nosql/data/2018110[6789]*.export.CSV.zip matches 0 files\n  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n  at java.lang.reflect.Method.invoke(Method.java:498)\n  at org.apache.zeppelin.spark.SparkZeppelinContext.showData(SparkZeppelinContext.java:108)\n  ... 74 more\nCaused by: org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input Pattern file:/home/palandir/nosql/data/2018110[6789]*.export.CSV.zip matches 0 files\n  at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:323)\n  at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:265)\n  at org.apache.spark.input.StreamFileInputFormat.setMinPartitions(PortableDataStream.scala:51)\n  at org.apache.spark.rdd.BinaryFileRDD.getPartitions(BinaryFileRDD.scala:51)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n  at scala.Option.getOrElse(Option.scala:121)\n  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:46)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n  at scala.Option.getOrElse(Option.scala:121)\n  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:46)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n  at scala.Option.getOrElse(Option.scala:121)\n  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:46)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n  at scala.Option.getOrElse(Option.scala:121)\n  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:46)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n  at scala.Option.getOrElse(Option.scala:121)\n  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:46)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n  at scala.Option.getOrElse(Option.scala:121)\n  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:46)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n  at scala.Option.getOrElse(Option.scala:121)\n  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:46)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n  at scala.Option.getOrElse(Option.scala:121)\n  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:46)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n  at scala.Option.getOrElse(Option.scala:121)\n  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:340)\n  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3278)\n  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489)\n  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489)\n  at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3259)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3258)\n  at org.apache.spark.sql.Dataset.head(Dataset.scala:2489)\n  at org.apache.spark.sql.Dataset.take(Dataset.scala:2703)\n  ... 79 more\n"}]},"apps":[],"jobName":"paragraph_1548333370006_1449830901","id":"20190123-211244_1276910069","dateCreated":"2019-01-24T13:36:10+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:262"},{"text":"// BATCHES MANUELS :\n\n// Status :  \n// DONE 110[01  6789]  KO  -2345-   \n// DONE 111[01234]\"    KO -567-89     \n// DONE 112[01234567 ] KO  --89\n// DONE 113[01]\"\n\n// DONE 100 2 679     KO  [1 3]\"45 8 9\n// DONE 101 2 679     KO [01 3]\"45 8 9\n// DONE 102 2 679        [01 3]\"45 8 9\n// DONE 103              [01]\" \n\nread_files_and_create_events_table(\"10[012][67]\")\n//read_files_and_create_mentions_table(moisJour)\n//writeRequete31ToMongo(moisJour)\nwriteRequete5ToMongo(\"10[012][67]\")\n","user":"anonymous","dateUpdated":"2019-01-24T15:26:39+0100","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Succès pour écriture Requête 5 (moisjour): 10[012][67]\n"}]},"apps":[],"jobName":"paragraph_1548333370007_-936061949","id":"20190122-124053_385491421","dateCreated":"2019-01-24T13:36:10+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:263"},{"user":"anonymous","dateUpdated":"2019-01-24T15:14:27+0100","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"results":{},"enabled":true,"editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1548333370008_1272759455","id":"20190123-214644_423113174","dateCreated":"2019-01-24T13:36:10+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:264","text":"%md\n\n# CONSOLIDATIONS (aka 2e etage)\n\nagglomérats/consolidations des batchs\n(travaille en overwrite)\n\ntrès rapide mais ne faire qu'une fois que tout est chargé !","dateFinished":"2019-01-24T15:14:27+0100","dateStarted":"2019-01-24T15:14:27+0100","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>CONSOLIDATIONS (aka 2e etage)</h1>\n<p>agglomérats/consolidations des batchs<br/>(travaille en overwrite)</p>\n<p>très rapide mais ne faire qu&rsquo;une fois que tout est chargé !</p>\n</div>"}]}},{"text":"// 3.1 > 3.2 en sparksql  et 3.1 > 4 en full scala\n\n// Ecriture Requete 32 (consolidation batches 31)\n\n// On charge la collection Requete3 depuis MongoDB\nval Requete31FromMongo = MongoSpark.\n                        load(sc, ReadConfig(Map(\"uri\" -> MONGO_URI_REQUETE31)) ).\n                        toDF()\n                        \nRequete31FromMongo.createOrReplaceTempView(\"TableRequete31\")\n\n\n// on regroupe \n// cette fois on a tout donc on peut generer des indicateurs qui n'etaient pas sommables dont le facteur de division\n\nval dfRequete32 = spark.sql(\"\"\"\nselect\nMois, CountryCode, Langue, Actor,\nActorCode, ActorCountryCode,\nsum(nbpos) as nbpos,\nsum(nbneg) as nbneg,\nsum(sommepos) as sommepos,\nsum(sommeneg) as sommeneg,\nabs(sum(nbpos)-sum(nbneg)) as deltanb,\nabs(sum(sommepos)-sum(sommeneg)) as deltasomme,\nsum(nbttl) as nbttl,\nabs(sum(nbpos)-sum(nbneg))/sum(nbttl) indic1,\nabs(sum(sommepos)-sum(sommeneg))/sum(nbttl) indic2\nfrom TableRequete31\ngroup by Mois, CountryCode, Langue, Actor, ActorCode, ActorCountryCode\n\"\"\").toDF()\n\nval MONGO_URI_REQUETE32 = MONGO_URI + \"Requete32\"\nMongoSpark.save(dfRequete32.write.mode(\"overwrite\"), WriteConfig(Map(\"uri\" -> MONGO_URI_REQUETE32)))\n\nprintln(\"Succès pour écriture Requête 32 (Global)\")\n\n// on a gardé le dataframe 3.1 donc on en profite pour ecrire la requette 4\n\nval dfRequete42 = dfRequete32.\n        //filter(dfRequete3b(\"nbpos\") > 0).\n        groupBy(\"Actor\").\n        agg(\n            sum(\"nbpos\").alias(\"cumnbpos\"),\n            sum(\"nbneg\").alias(\"cumnbneg\"),\n            sum(\"sommepos\").alias(\"cumsommepos\"),\n            sum(\"sommeneg\").alias(\"cumsommeneg\"),\n            sum(\"deltanb\").alias(\"cumdeltanb\"),\n            sum(\"deltasomme\").alias(\"cumdeltasomme\"),\n            sum(\"nbttl\").alias(\"ttl\")\n            ).\n        where($\"ttl\" > 1000). //corpus consequent remplacer 1000 par le bon seuil : si très segmentant mais juste sur 2 personnes 1 pour 1 contre alors c'est juste du bruit\n        withColumn(\"stat1\", abs($\"cumnbpos\" - $\"cumnbneg\")/$\"ttl\").\n        withColumn(\"stat2\", abs($\"cumsommepos\" - $\"cumsommeneg\")/$\"ttl\")\n        //.drop($\"ttl\").drop($\"delta\") // on garde quand meme à voir si en visu on veut\n\nval MONGO_URI_REQUETE42 = MONGO_URI + \"Requete42\"\nMongoSpark.save(dfRequete42.write.mode(\"overwrite\"), WriteConfig(Map(\"uri\" -> MONGO_URI_REQUETE42)))\n\nprintln(\"Succès pour écriture Requête 4 (Global)\")\n","user":"anonymous","dateUpdated":"2019-01-24T15:28:39+0100","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"dfRequete42: org.apache.spark.sql.DataFrame = [Actor: string, cumnbpos: bigint ... 8 more fields]\nMONGO_URI_REQUETE42: String = mongodb://127.0.0.1:27017/Gdelt.Requete42\nSuccès pour écriture Requête 4 (Global)\n"}]},"apps":[],"jobName":"paragraph_1548333370009_1682764903","id":"20190122-124104_780771289","dateCreated":"2019-01-24T13:36:10+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:265"},{"text":"// Ecriture Requete 5bis (consolidation batches du 5)\n\n// vu le niveau d'agglomerat voulu au mois on est sur qu'il y a des données à sommer vu la taille de nos batches\n// idem ya que là qu'on peut calculer les indicateurs non \"sommables\"\n\n// On charge la collection Requete5 depuis MongoDB\nval Requete5FromMongo = MongoSpark.\n                        load(sc, ReadConfig(Map(\"uri\" -> MONGO_URI_REQUETE5)) ).\n                        toDF()\n                        \nRequete5FromMongo.createOrReplaceTempView(\"TableRequete5\")\n\nval dfRequete5bis = spark.sql(\"\"\"\nselect\n  tt.Mois, tt.Actor1, tt.ActorCode1, tt.Actor2, tt.ActorCode2,\n  sum(nbpos) as nbpos,\n  sum(nbneg) as nbneg,\n  sum(sommepos) as sommepos,\n  sum(sommeneg) as sommeneg,\n  sum(nbttl) as nbttl,\n  sum(sommepos)-sum(sommeneg) as delta,\n  (sum(sommepos)-sum(sommeneg))/sum(nbttl) as avg\nfrom TableRequete5 tt\ngroup by tt.Mois, tt.Actor1, tt.ActorCode1, tt.Actor2, tt.ActorCode2\n\n\"\"\").toDF()\n\nval MONGO_URI_REQUETE5BIS = MONGO_URI + \"Requete5bis\"\nMongoSpark.save(dfRequete5bis.write.mode(\"overwrite\"), WriteConfig(Map(\"uri\" -> MONGO_URI_REQUETE5BIS)))\n\nprintln(\"Succès pour écriture Requête 5bis (Global)\")\n","user":"anonymous","dateUpdated":"2019-01-24T15:21:03+0100","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Requete5FromMongo: org.apache.spark.sql.DataFrame = [Actor1: string, Actor2: string ... 9 more fields]\ndfRequete5bis: org.apache.spark.sql.DataFrame = [Mois: string, Actor1: string ... 10 more fields]\nMONGO_URI_REQUETE5BIS: String = mongodb://172.31.33.253:27017,172.31.33.253:27018,172.31.44.85:27019,172.31.44.85:27020,172.31.47.231:27021,172.31.47.231:27022/Gdelt.Requete5bis\nSuccès pour écriture Requête 5bis (Global)\n"}]},"apps":[],"jobName":"paragraph_1548333370009_-1978145871","id":"20190123-124744_107782650","dateCreated":"2019-01-24T13:36:10+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:266"},{"text":"// DEBUG TEST de contenu table 5 cumul\n\nMongoSpark.load(sc, ReadConfig(Map(\"uri\" -> MONGO_URI_REQUETE5BIS)) ).toDF().createOrReplaceTempView(\"TableRequete5t\")\nz.show(spark.sql(\"\"\"\nselect Mois, count(1), ActorCode1\nfrom TableRequete5t\nwhere ActorCode1=\"USA\"\ngroup by Mois, ActorCode1\n\"\"\"))","user":"anonymous","dateUpdated":"2019-01-24T15:22:06+0100","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{"columns":[{"name":"Mois","visible":true,"width":"*","sort":{"priority":0,"direction":"asc"},"filters":[{}],"pinned":""},{"name":"count(1)","visible":true,"width":"*","sort":{},"filters":[{}],"pinned":""},{"name":"ActorCode1","visible":true,"width":"*","sort":{},"filters":[{}],"pinned":""}],"scrollFocus":{},"selection":[],"grouping":{"grouping":[],"aggregations":[],"rowExpandedStates":{}},"treeView":{},"pagination":{"paginationCurrentPage":1,"paginationPageSize":250}},"tableColumnTypeState":{"names":{"Mois":"string","count(1)":"string","ActorCode1":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false}},"commonSetting":{}}},"1":{"graph":{"mode":"table","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{"columns":[{"name":"Mois","visible":true,"width":"*","sort":{"priority":0,"direction":"asc"},"filters":[{}],"pinned":""},{"name":"count(1)","visible":true,"width":"*","sort":{},"filters":[{}],"pinned":""},{"name":"ActorCode1","visible":true,"width":"*","sort":{},"filters":[{}],"pinned":""}],"scrollFocus":{},"selection":[],"grouping":{"grouping":[],"aggregations":[],"rowExpandedStates":{}},"treeView":{},"pagination":{"paginationCurrentPage":1,"paginationPageSize":250}},"tableColumnTypeState":{"names":{"Mois":"string","count(1)":"string","ActorCode1":"string"},"updated":true},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false}},"commonSetting":{}}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"Mois\tcount(1)\tActorCode1\n200811\t5\tUSA\n201812\t32778\tUSA\n200812\t34\tUSA\n201811\t10566\tUSA\n201712\t1211\tUSA\n201810\t402\tUSA\n201711\t243\tUSA\n200901\t2\tUSA\n"}]},"apps":[],"jobName":"paragraph_1548333370010_-1483204581","id":"20190123-233718_1111327073","dateCreated":"2019-01-24T13:36:10+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:267"},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1548339766409_1106809292","id":"20190124-152246_106042057","dateCreated":"2019-01-24T15:22:46+0100","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:5345","text":"%md\n# Syntaxe initiale envisagée\n\nLe but etait d'avoir un gros batch qui tournerait la nuit\n\nDans la pratique on n'a jamais reussi à catcher les erreurs des jobs spark donc le chargement a été très manuel avec que des micro batch...\n\nde 3 minutes pour 3 jours de données quand ca passe ... de 10 minutes quand ca plante (sans savoir quel fichier isoler avant de relancer) ... mais à chaque fois il faut declencher à la main\n\n","dateUpdated":"2019-01-24T15:32:38+0100","dateFinished":"2019-01-24T15:32:38+0100","dateStarted":"2019-01-24T15:32:38+0100","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Syntaxe initiale envisagée</h1>\n<p>Le but etait d&rsquo;avoir un gros batch qui tournerait la nuit</p>\n<p>Dans la pratique on n&rsquo;a jamais reussi à catcher les erreurs des jobs spark donc le chargement a été très manuel avec que des micro batch&hellip;</p>\n<p>de 3 minutes pour 3 jours de données quand ca passe &hellip; de 10 minutes quand ca plante (sans savoir quel fichier isoler avant de relancer) &hellip; mais à chaque fois il faut declencher à la main</p>\n</div>"}]}},{"text":"// dommage qu'impossible de caster l'exception : ca rends impossible de scripter la boucle :\n\n// #TODO construite les pattern de fichier moisJour \n// à l'origine ce devait etre des batch d'un mois : (\"01\",\"02\",...,\"12\")\n// dans la pratique on est plus sur du jour(s) : donc des listes dont les elems sont du type \"121[0123]\" pour du 10 au 13 decembre\n\n// loop sur patterns\n\n    // substitut à la bouche : un seul pattern pour un batch pour le 10 et 11 decembre avec toutes les requetes\n    val moisJour = \"121[01]\"\n\n    read_events_and_mentions_table_ByDate(moisJour)\n    writeRequete1ToMongo(moisJour)\n    writeRequete2ToMongo(moisJour)\n    writeRequete3ToMongo(moisJour)\n    writeRequete5ToMongo(moisJour)\n// end loop\n\n\n\n// lancer la cellule de cumul 3.2(et4) et 5BIS\n\n","user":"anonymous","dateUpdated":"2019-01-24T15:34:52+0100","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1548333370011_-752653158","id":"20190121-124600_330712567","dateCreated":"2019-01-24T13:36:10+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:270"}],"name":"Gdelt-AWS-S3-2Mongo","id":"2E2M687AW","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}